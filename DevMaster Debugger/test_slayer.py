#!/usr/bin/env python3
"""
Test-Slayer: Autonomous Test Fixing System

Fixes failing tests automatically by:
- Analyzing test failures
- Understanding test patterns
- Generating appropriate fixes
- Validating fixes work

Handles:
- Assertion errors
- Missing mocks
- Outdated fixtures
- Flaky tests
- Import errors
- Type errors in tests
"""

import ast
import re
import subprocess
import sys
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum


class FailureType(Enum):
    """Types of test failures we can fix"""
    ASSERTION_ERROR = "assertion_error"
    MISSING_MOCK = "missing_mock"
    IMPORT_ERROR = "import_error"
    ATTRIBUTE_ERROR = "attribute_error"
    TYPE_ERROR = "type_error"
    FIXTURE_ERROR = "fixture_error"
    TIMEOUT = "timeout"
    FLAKY = "flaky"
    UNKNOWN = "unknown"


@dataclass
class TestFailure:
    """Represents a test failure"""
    test_name: str
    failure_type: FailureType
    error_message: str
    stack_trace: List[str]
    file_path: str
    line_number: Optional[int]
    context: Dict[str, Any]


@dataclass
class TestFix:
    """Represents a fix for a test"""
    original_code: str
    fixed_code: str
    explanation: str
    confidence: float


class TestRunner:
    """Runs tests and captures output"""

    def __init__(self):
        self.framework = None  # Detect pytest, unittest, etc.

    def detect_framework(self, test_file: Path) -> str:
        """Detect which test framework is being used"""
        content = test_file.read_text()

        if 'import pytest' in content or '@pytest' in content:
            return 'pytest'
        elif 'import unittest' in content or 'unittest.TestCase' in content:
            return 'unittest'
        elif 'from django.test' in content:
            return 'django'
        else:
            return 'pytest'  # Default assumption

    def run_test(self, test_file: Path, test_name: Optional[str] = None) -> Tuple[bool, str]:
        """Run test and return (passed, output)"""
        self.framework = self.detect_framework(test_file)

        if self.framework == 'pytest':
            cmd = ['pytest', str(test_file), '-v', '--tb=short']
            if test_name:
                cmd.append(f'-k {test_name}')
        elif self.framework == 'unittest':
            cmd = ['python', '-m', 'unittest', str(test_file)]
        else:
            cmd = ['pytest', str(test_file), '-v']

        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )
            passed = result.returncode == 0
            output = result.stdout + result.stderr
            return passed, output
        except subprocess.TimeoutExpired:
            return False, "Test timed out after 30 seconds"
        except Exception as e:
            return False, f"Error running test: {str(e)}"


class FailureAnalyzer:
    """Analyzes test failures to determine type and context"""

    def analyze(self, output: str, test_file: Path) -> List[TestFailure]:
        """Parse test output and extract failures"""
        failures = []

        # Parse pytest output
        if 'FAILED' in output or 'ERROR' in output:
            failures.extend(self._parse_pytest_output(output, test_file))

        # Parse unittest output
        if 'FAIL:' in output or 'ERROR:' in output:
            failures.extend(self._parse_unittest_output(output, test_file))

        return failures

    def _parse_pytest_output(self, output: str, test_file: Path) -> List[TestFailure]:
        """Parse pytest failure output"""
        failures = []

        # Split into individual test failures
        failure_sections = re.split(r'_{70,}', output)

        for section in failure_sections:
            if 'FAILED' not in section and 'ERROR' not in section:
                continue

            failure = self._extract_failure_info(section, test_file)
            if failure:
                failures.append(failure)

        return failures

    def _parse_unittest_output(self, output: str, test_file: Path) -> List[TestFailure]:
        """Parse unittest failure output"""
        # Similar to pytest but with unittest patterns
        return self._parse_pytest_output(output, test_file)

    def _extract_failure_info(self, section: str, test_file: Path) -> Optional[TestFailure]:
        """Extract failure information from test output section"""

        # Extract test name
        test_name_match = re.search(r'test_\w+', section)
        if not test_name_match:
            return None
        test_name = test_name_match.group(0)

        # Determine failure type
        failure_type = self._classify_failure(section)

        # Extract error message
        error_message = self._extract_error_message(section)

        # Extract stack trace
        stack_trace = self._extract_stack_trace(section)

        # Extract line number
        line_number = self._extract_line_number(section)

        # Build context
        context = self._build_context(section, failure_type)

        return TestFailure(
            test_name=test_name,
            failure_type=failure_type,
            error_message=error_message,
            stack_trace=stack_trace,
            file_path=str(test_file),
            line_number=line_number,
            context=context
        )

    def _classify_failure(self, section: str) -> FailureType:
        """Classify the type of failure"""
        section_lower = section.lower()

        if 'assertionerror' in section_lower or 'assert' in section_lower:
            return FailureType.ASSERTION_ERROR
        elif 'importerror' in section_lower or 'modulenotfounderror' in section_lower:
            return FailureType.IMPORT_ERROR
        elif 'attributeerror' in section_lower:
            return FailureType.ATTRIBUTE_ERROR
        elif 'typeerror' in section_lower:
            return FailureType.TYPE_ERROR
        elif 'fixture' in section_lower:
            return FailureType.FIXTURE_ERROR
        elif 'timeout' in section_lower:
            return FailureType.TIMEOUT
        else:
            return FailureType.UNKNOWN

    def _extract_error_message(self, section: str) -> str:
        """Extract the error message"""
        lines = section.split('\n')
        for line in lines:
            if any(err in line for err in ['Error:', 'AssertionError:', 'assert ']):
                return line.strip()
        return "Unknown error"

    def _extract_stack_trace(self, section: str) -> List[str]:
        """Extract stack trace lines"""
        lines = section.split('\n')
        trace = []
        in_trace = False

        for line in lines:
            if 'Traceback' in line or 'File "' in line:
                in_trace = True
            if in_trace:
                trace.append(line)

        return trace

    def _extract_line_number(self, section: str) -> Optional[int]:
        """Extract line number from error"""
        match = re.search(r'line (\d+)', section)
        if match:
            return int(match.group(1))
        return None

    def _build_context(self, section: str, failure_type: FailureType) -> Dict[str, Any]:
        """Build context dictionary for the failure"""
        context = {'raw_output': section}

        if failure_type == FailureType.ASSERTION_ERROR:
            # Extract expected vs actual
            expected = re.search(r'expected:?\s*(.+)', section, re.IGNORECASE)
            actual = re.search(r'actual:?\s*(.+)', section, re.IGNORECASE)
            if expected:
                context['expected'] = expected.group(1).strip()
            if actual:
                context['actual'] = actual.group(1).strip()

            # Also try to extract from assertion comparison
            assert_match = re.search(r'assert (.+?) == (.+)', section)
            if assert_match:
                context['left'] = assert_match.group(1).strip()
                context['right'] = assert_match.group(2).strip()

        elif failure_type == FailureType.IMPORT_ERROR:
            # Extract module name
            module_match = re.search(r'No module named [\'"](.+?)[\'"]', section)
            if module_match:
                context['missing_module'] = module_match.group(1)

        elif failure_type == FailureType.ATTRIBUTE_ERROR:
            # Extract attribute and object
            attr_match = re.search(r"'(\w+)' object has no attribute '(\w+)'", section)
            if attr_match:
                context['object_type'] = attr_match.group(1)
                context['missing_attribute'] = attr_match.group(2)

        return context


class FixGenerator:
    """Generates fixes for test failures"""

    def __init__(self):
        self.ast_cache = {}

    def generate_fix(self, failure: TestFailure, test_file: Path) -> Optional[TestFix]:
        """Generate a fix for the given failure"""

        # Get test file AST
        test_ast = self._get_ast(test_file)
        if not test_ast:
            return None

        # Dispatch to specific fix generator
        if failure.failure_type == FailureType.ASSERTION_ERROR:
            return self._fix_assertion_error(failure, test_file, test_ast)
        elif failure.failure_type == FailureType.MISSING_MOCK:
            return self._fix_missing_mock(failure, test_file, test_ast)
        elif failure.failure_type == FailureType.IMPORT_ERROR:
            return self._fix_import_error(failure, test_file, test_ast)
        elif failure.failure_type == FailureType.ATTRIBUTE_ERROR:
            return self._fix_attribute_error(failure, test_file, test_ast)
        elif failure.failure_type == FailureType.TYPE_ERROR:
            return self._fix_type_error(failure, test_file, test_ast)
        elif failure.failure_type == FailureType.TIMEOUT:
            return self._fix_timeout(failure, test_file, test_ast)
        else:
            return None

    def _get_ast(self, file_path: Path) -> Optional[ast.AST]:
        """Get or cache AST for file"""
        if file_path in self.ast_cache:
            return self.ast_cache[file_path]

        try:
            content = file_path.read_text()
            tree = ast.parse(content)
            self.ast_cache[file_path] = tree
            return tree
        except Exception as e:
            print(f"Error parsing {file_path}: {e}")
            return None

    def _fix_assertion_error(self, failure: TestFailure, test_file: Path, test_ast: ast.AST) -> Optional[TestFix]:
        """Fix assertion errors by updating expected values"""

        original_code = test_file.read_text()
        lines = original_code.split('\n')

        if not failure.line_number:
            return None

        # Get the failing assertion line
        assert_line_idx = failure.line_number - 1
        if assert_line_idx >= len(lines):
            return None

        assert_line = lines[assert_line_idx]

        # Check if this is an attribute error disguised as assertion
        if failure.context.get('object_type') and failure.context.get('missing_attribute'):
            return self._fix_attribute_error(failure, test_file, test_ast)

        # Extract expected and actual from context
        expected = failure.context.get('expected')
        actual = failure.context.get('actual')

        if not expected or not actual:
            # Try alternative extraction
            left = failure.context.get('left')
            right = failure.context.get('right')
            if left and right:
                # This is a comparison, suggest to check which side is wrong
                return TestFix(
                    original_code=original_code,
                    fixed_code=original_code,
                    explanation=f"Assertion failed: {left} != {right}. Review which value should change.",
                    confidence=0.3
                )
            return None

        # Replace expected with actual in the assertion
        fixed_line = assert_line.replace(str(expected), str(actual))

        # Build fixed code
        fixed_lines = lines.copy()
        fixed_lines[assert_line_idx] = fixed_line
        fixed_code = '\n'.join(fixed_lines)

        return TestFix(
            original_code=original_code,
            fixed_code=fixed_code,
            explanation=f"Updated assertion: expected value changed from {expected} to {actual}",
            confidence=0.8
        )

    def _fix_missing_mock(self, failure: TestFailure, test_file: Path, test_ast: ast.AST) -> Optional[TestFix]:
        """Add missing mocks for dependencies"""

        original_code = test_file.read_text()

        # Find the test function
        test_func = self._find_test_function(test_ast, failure.test_name)
        if not test_func:
            return None

        # Detect what needs to be mocked from the error
        # This is a simplified version - real implementation would analyze imports and calls

        mock_code = """from unittest.mock import Mock, patch

"""

        fixed_code = mock_code + original_code

        return TestFix(
            original_code=original_code,
            fixed_code=fixed_code,
            explanation="Added mock imports and setup",
            confidence=0.6
        )

    def _fix_import_error(self, failure: TestFailure, test_file: Path, test_ast: ast.AST) -> Optional[TestFix]:
        """Fix import errors"""

        original_code = test_file.read_text()
        missing_module = failure.context.get('missing_module')

        if not missing_module:
            return None

        # Try to fix common import issues
        fixes = {
            'mock': 'from unittest import mock',
            'pytest': 'import pytest',
            'unittest.mock': 'from unittest import mock',
        }

        if missing_module in fixes:
            # Check if import already exists
            if fixes[missing_module] in original_code:
                return None

            # Find where to insert import (after other imports)
            lines = original_code.split('\n')
            insert_pos = 0
            for i, line in enumerate(lines):
                if line.startswith('import ') or line.startswith('from '):
                    insert_pos = i + 1
                elif line.strip() and not line.startswith('#'):
                    break

            lines.insert(insert_pos, fixes[missing_module])
            fixed_code = '\n'.join(lines)

            return TestFix(
                original_code=original_code,
                fixed_code=fixed_code,
                explanation=f"Added missing import for {missing_module}",
                confidence=0.9
            )

        return None

    def _fix_attribute_error(self, failure: TestFailure, test_file: Path, test_ast: ast.AST) -> Optional[TestFix]:
        """Fix attribute errors by suggesting correct attribute name"""

        original_code = test_file.read_text()
        lines = original_code.split('\n')

        object_type = failure.context.get('object_type')
        missing_attr = failure.context.get('missing_attribute')

        if not object_type or not missing_attr:
            return None

        if not failure.line_number:
            return None

        # Get the failing line
        line_idx = failure.line_number - 1
        if line_idx >= len(lines):
            return None

        failing_line = lines[line_idx]

        # Try to suggest common attribute name fixes
        common_fixes = {
            'email': 'email_address',
            'name': 'full_name',
            'id': 'pk',
            'is_active': 'active',
        }

        suggested_attr = common_fixes.get(missing_attr, missing_attr + '_field')

        # Replace the attribute
        fixed_line = failing_line.replace(f'.{missing_attr}', f'.{suggested_attr}')

        fixed_lines = lines.copy()
        fixed_lines[line_idx] = fixed_line
        fixed_code = '\n'.join(fixed_lines)

        return TestFix(
            original_code=original_code,
            fixed_code=fixed_code,
            explanation=f"Changed attribute from '{missing_attr}' to '{suggested_attr}' (common pattern)",
            confidence=0.6
        )

    def _fix_type_error(self, failure: TestFailure, test_file: Path, test_ast: ast.AST) -> Optional[TestFix]:
        """Fix type errors in tests"""

        original_code = test_file.read_text()

        # Analyze the type error and suggest fixes
        # This would involve understanding expected types and converting

        return TestFix(
            original_code=original_code,
            fixed_code=original_code,
            explanation="Type error detected - manual review needed",
            confidence=0.3
        )

    def _fix_timeout(self, failure: TestFailure, test_file: Path, test_ast: ast.AST) -> Optional[TestFix]:
        """Fix timeout issues by adding retries or increasing timeouts"""

        original_code = test_file.read_text()
        lines = original_code.split('\n')

        # Find the test function and add timeout decorator or retry logic
        test_func = self._find_test_function(test_ast, failure.test_name)
        if not test_func:
            return None

        # Check if pytest is available
        if 'import pytest' not in original_code:
            lines.insert(0, 'import pytest')

        # Add pytest timeout marker
        test_func_line = test_func.lineno - 1

        decorator_line = "@pytest.mark.timeout(60)  # Increased timeout"
        lines.insert(test_func_line, decorator_line)

        fixed_code = '\n'.join(lines)

        return TestFix(
            original_code=original_code,
            fixed_code=fixed_code,
            explanation="Added timeout marker to prevent flaky timeout failures",
            confidence=0.7
        )

    def _find_test_function(self, tree: ast.AST, test_name: str) -> Optional[ast.FunctionDef]:
        """Find a test function in the AST"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef) and node.name == test_name:
                return node
        return None


class TestSlayer:
    """Main test fixing orchestrator"""

    def __init__(self):
        self.runner = TestRunner()
        self.analyzer = FailureAnalyzer()
        self.generator = FixGenerator()
        self.max_attempts = 3

    def fix_test_file(self, test_file: str) -> Dict[str, Any]:
        """Fix all failing tests in a file"""

        test_path = Path(test_file)
        if not test_path.exists():
            return {"error": f"Test file not found: {test_file}"}

        print(f"\n{'='*70}")
        print(f"üó°Ô∏è  Test-Slayer: Analyzing {test_path.name}")
        print(f"{'='*70}\n")

        # Run tests initially
        passed, output = self.runner.run_test(test_path)

        if passed:
            return {
                "status": "already_passing",
                "file": str(test_path),
                "message": "‚úÖ All tests already passing"
            }

        print("‚ùå Tests failing. Analyzing failures...\n")

        # Analyze failures
        failures = self.analyzer.analyze(output, test_path)

        if not failures:
            return {
                "status": "no_failures_detected",
                "file": str(test_path),
                "output": output
            }

        print(f"Found {len(failures)} failure(s):\n")
        for i, failure in enumerate(failures, 1):
            print(f"{i}. {failure.test_name}")
            print(f"   Type: {failure.failure_type.value}")
            print(f"   Error: {failure.error_message[:100]}...")
            print()

        # Attempt to fix
        results = self._fix_failures(failures, test_path)

        return {
            "status": "completed",
            "file": str(test_path),
            "failures_found": len(failures),
            "fixes_attempted": len(results['fixes']),
            "fixes_successful": results['successful'],
            "final_passed": results['final_passed'],
            "details": results
        }

    def _fix_failures(self, failures: List[TestFailure], test_path: Path) -> Dict[str, Any]:
        """Attempt to fix all failures"""

        fixes_applied = []
        successful = 0
        attempt = 0

        # Save original content for potential rollback
        original_content = test_path.read_text()

        while attempt < self.max_attempts:
            print(f"\nüîß Attempt {attempt + 1}/{self.max_attempts}\n")

            for failure in failures:
                print(f"Generating fix for {failure.test_name}...")

                fix = self.generator.generate_fix(failure, test_path)

                if not fix:
                    print(f"  ‚ö†Ô∏è  Could not generate fix (confidence too low)")
                    continue

                if fix.confidence < 0.5:
                    print(f"  ‚ö†Ô∏è  Low confidence fix ({fix.confidence:.1%})")
                    continue

                print(f"  ‚úì Fix generated (confidence: {fix.confidence:.1%})")
                print(f"    {fix.explanation}")

                # Apply fix
                test_path.write_text(fix.fixed_code)
                fixes_applied.append({
                    'test': failure.test_name,
                    'type': failure.failure_type.value,
                    'explanation': fix.explanation,
                    'confidence': fix.confidence
                })

                # Re-run test
                print(f"  Testing fix...")
                passed, output = self.runner.run_test(test_path, failure.test_name)

                if passed:
                    print(f"  ‚úÖ Test now passing!")
                    successful += 1
                else:
                    print(f"  ‚ùå Test still failing")
                    # Revert fix if it didn't work
                    test_path.write_text(fix.original_code)

            # Check if all tests pass now
            passed, output = self.runner.run_test(test_path)
            if passed:
                print(f"\n{'='*70}")
                print(f"‚úÖ ALL TESTS PASSING AFTER {attempt + 1} ATTEMPT(S)")
                print(f"{'='*70}\n")
                break

            # Re-analyze remaining failures
            failures = self.analyzer.analyze(output, test_path)
            if not failures:
                break

            attempt += 1

        # Final test run
        final_passed, final_output = self.runner.run_test(test_path)

        return {
            'fixes': fixes_applied,
            'successful': successful,
            'attempts': attempt + 1,
            'final_passed': final_passed,
            'final_output': final_output
        }


def main():
    """CLI entry point"""
    if len(sys.argv) < 2:
        print("üó°Ô∏è  Test-Slayer: Your tests fix themselves\n")
        print("Usage: python test_slayer.py <test_file>")
        print("\nExample:")
        print("  python test_slayer.py tests/test_example.py")
        sys.exit(1)

    test_file = sys.argv[1]

    slayer = TestSlayer()
    result = slayer.fix_test_file(test_file)

    # Print final summary
    print("\n" + "="*70)
    print("üìä FINAL RESULTS")
    print("="*70)

    if result.get('final_passed'):
        print("‚úÖ Status: ALL TESTS PASSING")
        print(f"‚úÖ Fixes applied: {result.get('fixes_attempted', 0)}")
        print(f"‚úÖ Success rate: {result.get('fixes_successful', 0)}/{result.get('failures_found', 0)}")
    else:
        print("‚ö†Ô∏è  Status: Some tests still failing")
        print(f"‚ö†Ô∏è  Fixes attempted: {result.get('fixes_attempted', 0)}")
        print(f"‚ö†Ô∏è  Fixes successful: {result.get('fixes_successful', 0)}/{result.get('failures_found', 0)}")

    print("\nDetails:")
    print(json.dumps(result, indent=2, default=str))

    if result.get('final_passed'):
        sys.exit(0)
    else:
        sys.exit(1)


if __name__ == '__main__':
    main()
